{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2.4 Regressió logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índex\n",
    "\n",
    "1. Codificació one-hot i distribució categòrica\n",
    "2. Model probabilístic de classificació amb softmax\n",
    "3. Regressió logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Codificació one-hot i distribució categòrica\n",
    "\n",
    "**Variable categòrica:** $\\;$ variable aleatòria que pren un valor d'un conjunt finit de categories (no ordenades)\n",
    "\n",
    "**Exemples de variables categòriques:** $\\;$ color RGB, **etiqueta de classe,** paraula d'un vocabulari, etc.\n",
    "\n",
    "**Codificació one-hot:** $\\;$ d'una variable categòrica $y$ que pren un valor entre $C$ possibles, $\\,\\{1,\\dotsc,C\\}$\n",
    "$$\\operatorname{one-hot}(y)%\n",
    "=\\boldsymbol{y}%\n",
    "=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_C\\end{pmatrix}%\n",
    "=\\begin{pmatrix}\\mathbb{I}(y=1)\\\\\\vdots\\\\\\mathbb{I}(y=C)\\end{pmatrix}\\in\\{0,1\\}^C%\n",
    "\\quad\\text{amb}\\quad%\n",
    "\\sum_c y_c=1$$\n",
    "\n",
    "**Distribució categòrica:** $\\;$ distribució de probabilitats entre les $C$ possibles categories d'una variable categòrica, les probabilitats de les quals venen donades per un vector paràmetres $\\,\\boldsymbol{\\theta}\\in[0,1]^C\\,$ tal que $\\,\\sum_c\\theta_c=1$\n",
    "$$\\operatorname{Cat}(y\\mid\\boldsymbol{\\theta})=\\prod_{c=1}^C\\theta_c^{\\mathbb{I}(y=c)}%\n",
    "\\qquad\\text{o, en notació one-hot,}\\qquad%\n",
    "\\operatorname{Cat}(\\boldsymbol{y}\\mid\\boldsymbol{\\theta})=\\prod_{c=1}^C\\theta_c^{y_c}$$\n",
    "\n",
    "**Convenció:** $\\;0^0=1\\;$ i $\\;0\\log 0=0;\\;$ per exemple, amb $\\,\\boldsymbol{\\theta}=(0.5, 0.5, 0)^t,\\,\\operatorname{Cat}(\\boldsymbol{y}=(1,0,0)^t\\mid\\boldsymbol{\\theta})=0.5^10.5^00^0=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model probabilístic de classificació amb softmax\n",
    "\n",
    "**Normalització probabilística de classificadors:** $\\;$ tot classificador definit amb funcions discriminants generals pot representar-se mitjançant un classificador equivalent amb funcions discriminants normalitzades probabilísticament\n",
    "$$\\begin{align*}\n",
    "c(\\boldsymbol{x})%\n",
    "&=\\operatorname*{argmax}\\limits_c\\;a_c && \\text{on $\\,a_c\\,$ és la discriminant de la classe $c$ avaluada a $\\boldsymbol{x}$}\\\\%\n",
    "&=\\operatorname*{argmax}\\limits_c\\;e^{a_c} && \\text{amb}\\;h(z)=e^z\\in\\mathbb{R}^{\\geq 0}\\;\\text{estrictament creixent}\\\\%\n",
    "&=\\operatorname*{argmax}\\limits_c\\;\\frac{e^{a_c}}{\\sum_{c'} e^{a_{c'}}} && \\text{amb}\\;h(z)=kz,\\,k\\;\\text{constant positiva (invariable amb $c$)}%\n",
    "\\end{align*}$$\n",
    "\n",
    "**La funció softmax:** $\\;$ transforma un vector de **logits** (log-probabilitats no normalitzades) $\\,\\boldsymbol{a}\\in\\mathbb{R}^C\\,$ en un de probabilitats $[0,1]^C$\n",
    "$$\\mathcal{S}(\\boldsymbol{a})=\\left[%\n",
    "\\frac{e^{a_1}}{\\sum_{\\tilde{c}}e^{a_{\\tilde{c}}}},\\dotsc,%\n",
    "\\frac{e^{a_C}}{\\sum_{\\tilde{c}}e^{a_{\\tilde{c}}}}\\right]%\n",
    "\\qquad\\text{complint-se}\\qquad%\n",
    "0\\leq\\mathcal{S}(\\boldsymbol{a})_c\\leq 1%\n",
    "\\quad\\text{i}\\quad%\n",
    "\\sum_c \\mathcal{S}(\\boldsymbol{a})_c=1$$\n",
    "\n",
    "**Model probabilístic de classificació amb softmax:** $\\;$ en comptes de predir una única classe més probable, predim les probabilitats de totes les classes a partir d'una funció predictora de logits, $\\,f:\\mathcal{X}\\to\\mathbb{R}^C,\\,$ governada per un vector de paràmetres $\\,\\boldsymbol{\\theta}$\n",
    "$$p(\\boldsymbol{y}\\mid\\boldsymbol{x},\\boldsymbol{\\theta})%\n",
    "=\\operatorname{Cat}(\\boldsymbol{y}\\mid\\mathcal{S}(f(\\boldsymbol{x};\\boldsymbol{\\theta})))%\n",
    "=\\prod_c\\mathcal{S}(f(\\boldsymbol{x};\\boldsymbol{\\theta}))_c^{y_c}$$\n",
    "\n",
    "**Conveniència del model en inferència:** $\\;$ la predicció de les probabilitats de totes les classes permet aplicar regles més generals que la MAP, per exemple en cas d'errors amb costs diferents; a més, si volem aplicar la regla MAP, no cal softmax-normalitzar logits\n",
    "\n",
    "**Conveniència del model en aprenentatge:** $\\;$ permet plantejar l'aprenentatge probabilísticament, amb criteris estàndard com ara màxima versemblança; a més, gràcies a la softmax, $\\,f(\\boldsymbol{x};\\boldsymbol{\\theta})\\,$\n",
    "pot triar-se lliurement ja que no està subjecta a les restriccions de probabilitat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
